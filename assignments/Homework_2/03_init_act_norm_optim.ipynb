{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализайия и нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит реализовать два вида нормализации: по батчам (BatchNorm1d) и по признакам (LayerNorm1d)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:24:34.779670Z",
     "start_time": "2024-09-23T10:24:34.763473Z"
    }
   },
   "source": [
    "from typing import Callable, NamedTuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Реализация BatchNorm1d и LayerNorm1d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 1.1. (2 балла) Реализуйте BatchNorm1d\n",
    "\n",
    "Подсказка: чтобы хранить текущие значения среднего и дисперсии, вам потребуется метод `torch.nn.Module.register_buffer`, ознакомьтесь с документацией к нему. Подумайте, какие проблемы возникнут, если вы будете просто сохранять ваши значения в тензор"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:24:34.799221Z",
     "start_time": "2024-09-23T10:24:34.787732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BatchNorm1d(nn.Module):\n",
    "    def __init__(self, num_features: int, momentum: float = 0.9, eps: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(num_features))\n",
    "        self.shift = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_var\", torch.ones(num_features))\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(dim=0)\n",
    "            batch_var = x.var(dim=0)\n",
    "\n",
    "            self.running_mean = (1 - self.momentum) * batch_mean + self.momentum * self.running_mean\n",
    "            self.running_var = (1 - self.momentum) * batch_var + self.momentum * self.running_var\n",
    "\n",
    "            x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "        else:\n",
    "            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
    "\n",
    "        return self.scale * x_normalized + self.shift"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. (1 балл) Реализуйте LayerNorm1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия LayerNorm от BatchNorm - в том, что расчёт средних и дисперсий в BatchNorm происходит вдоль размерности батча (см. рисунок слева), а в LayerNorm - вдоль размерности признаков (см. рисунок справа).\n",
    "\n",
    "<img src=\"norm.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:24:34.817195Z",
     "start_time": "2024-09-23T10:24:34.811367Z"
    }
   },
   "source": [
    "class LayerNorm1d(nn.Module):\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5) -> None:\n",
    "        super(LayerNorm1d, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(num_features))\n",
    "        self.shift = nn.Parameter(torch.zeros(num_features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        \n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        return self.scale * x_normalized + self.shift"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании ваша задача - проверить, какие из приёмов хорошо справляются с нездоровыми активациями в промежуточных слоях. Вам будет дана базовая модель, у которой есть проблемы с инициализацией параметров, попробуйте несколько приёмов для устранения проблем обучения:\n",
    "1. Хорошая инициализация параметров\n",
    "2. Ненасыщаемая функция активации (например, `F.leaky_relu`)\n",
    "3. Нормализация по батчам или по признакам (можно использовать встроенные `nn.BatchNorm1d` и `nn.LayerNorm`)\n",
    "4. Более продвинутый оптимизатор (`torch.optim.RMSprop`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0. Подготовка: датасет, функции для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверять наши гипотезы будем на датасете MNIST, для отладки добавим в функции для обучения возможность использовать только несколько батчей данных"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:24:34.881098Z",
     "start_time": "2024-09-23T10:24:34.826924Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:24:34.904217Z",
     "start_time": "2024-09-23T10:24:34.894434Z"
    }
   },
   "source": [
    "def training_step(\n",
    "    batch: tuple[torch.Tensor, torch.Tensor],\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> torch.Tensor:\n",
    "    # прогоняем батч через модель\n",
    "    x, y = batch\n",
    "    logits = model(x)\n",
    "    # оцениваем значение ошибки\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    # обновляем параметры\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # возвращаем значение функции ошибки для логирования\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    max_batches: int = 100,\n",
    ") -> Tensor:\n",
    "    loss_values: list[float] = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        loss = training_step(batch, model, optimizer)\n",
    "        loss_values.append(loss.item())\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    return torch.tensor(loss_values).mean()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_epoch(\n",
    "    dataloader: DataLoader, model: nn.Module, max_batches: int = 100\n",
    ") -> Tensor:\n",
    "    loss_values: list[float] = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = batch\n",
    "        logits = model(x)\n",
    "        # оцениваем значение ошибки\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss_values.append(loss.item())\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    return torch.tensor(loss_values).mean()"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Определение класса модели (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства проведения экспериментов мы немного усложним создание модели, чтобы можно было задать разные способы инициализации параметров и нормализации промежуточных активаций, не меняя определение класса.\n",
    "\n",
    "Добавьте в метод `__init__`:\n",
    "- аргумент, который позволит использовать разные функции активации для промежуточных слоёв\n",
    "- аргумент, который позволит задавать разные способы нормализации: `None` (без нормализации), `nn.BatchNorm` и `nn.LayerNorm`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:24:34.931812Z",
     "start_time": "2024-09-23T10:24:34.921009Z"
    }
   },
   "source": [
    "def init_std_normal(model: nn.Module) -> None:\n",
    "    \"\"\"Функция для инициализации параметров модели стандартным нормальным распределением.\"\"\"\n",
    "    for param in model.parameters():\n",
    "        torch.nn.init.normal_(param.data, mean=0, std=1)\n",
    "\n",
    "\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Базовая модель для экспериментов\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): размерность входных признаков\n",
    "        hidden_dim (int): размерност скрытого слоя\n",
    "        output_dim (int): кол-во классов\n",
    "        act_fn (Callable[[Tensor], Tensor], optional): Функция активации. Defaults to F.tanh.\n",
    "        init_fn (Callable[[nn.Module], None], optional): Функция для инициализации. Defaults to init_std_normal.\n",
    "        norm (Type[nn.BatchNorm1d  |  nn.LayerNorm] | None, optional): Способ нормализации промежуточных активаций.\n",
    "            Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        act_fn: Callable[[Tensor], Tensor] = F.tanh,\n",
    "        init_fn: Callable[[nn.Module], None] = init_std_normal,\n",
    "        norm: Type[nn.BatchNorm1d | nn.LayerNorm] | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # теперь линейные слои будем задавать\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act_fn = act_fn\n",
    "        self.norm = norm(hidden_dim) if norm else None\n",
    "\n",
    "        # reinitialize parameters\n",
    "        init_fn(self)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = self.fc1.forward(x.flatten(1))\n",
    "        # here you can do normalization\n",
    "        if self.norm:\n",
    "            h = self.norm(h)\n",
    "        return self.fc2.forward(self.act_fn(h))"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Эксперименты (7 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите по 3 эксперимента с каждой из модификаций с разными значениями `seed`, соберите статистику значений тестовой ошибки после 10 эпох обучения, сделайте выводы о том, что работает лучше\n",
    "\n",
    "Проверяем:\n",
    "1. Метод инициализации весов модели: $\\mathcal{N}(0, 1)$ / Kaiming normal\n",
    "2. Функция активации: tanh /  (или любая другая без насыщения)\n",
    "3. Слой нормализации: None / BatchNorm / LayerNorm\n",
    "4. Выбранный оптимизатор: SGD / RMSprop / Adam\n",
    "\n",
    "Итого у нас 2 + 2 + 3 + 3 = 10 экспериментов, каждый нужно повторить 3 раза, посчитать среднее и вывести результаты в pandas.DataFrame.\n",
    "Можно дополнительно потестировать разные сочетания опций, например инициализация + нормализация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы автоматизировать проведение экспериментов, можно использовать функцию, которая будет принимать все необходимые настройки эксперимента, запускать его и сохранять нужные метрики:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:24:34.958030Z",
     "start_time": "2024-09-23T10:24:34.944959Z"
    }
   },
   "source": [
    "def run_experiment(\n",
    "    model_gen: Callable[[], nn.Module],\n",
    "    optim_gen: Callable[[nn.Module], torch.optim.Optimizer],\n",
    "    seed: int,\n",
    "    n_epochs: int = 10,\n",
    "    max_batches: int | None = None,\n",
    "    verbose: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"Функция для запуска экспериментов.\n",
    "\n",
    "    Args:\n",
    "        model_gen (Callable[[], nn.Module]): Функция для создания модели\n",
    "        optim_gen (Callable[[nn.Module], torch.optim.Optimizer]): Функция для создания оптимизатора для модели\n",
    "        seed (int): random seed\n",
    "        n_epochs (int, optional): Число эпох обучения. Defaults to 10.\n",
    "        max_batches (int | None, optional): Если указано, только `max_batches` минибатчей\n",
    "            будет использоваться при обучении и тестировании. Defaults to None.\n",
    "        verbose (bool, optional): Выводить ли информацию для отладки. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        float: Значение ошибки на тестовой выборке в конце обучения\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    # создадим модель и выведем значение ошибки после инициализации\n",
    "    model = model_gen()\n",
    "    optim = optim_gen(model)\n",
    "    epoch_losses: list[float] = []\n",
    "    for i in range(n_epochs):\n",
    "        train_loss = train_epoch(train_loader, model, optim, max_batches=max_batches)\n",
    "        test_loss = test_epoch(test_loader, model, max_batches=max_batches)\n",
    "        if verbose:\n",
    "            print(f\"Epoch {i} train loss = {train_loss:.4f}\")\n",
    "            print(f\"Epoch {i} test loss = {test_loss:.4f}\")\n",
    "\n",
    "        epoch_losses.append(test_loss.item())\n",
    "\n",
    "    last_epoch_loss = epoch_losses[-1]\n",
    "    return last_epoch_loss"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример использования:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:24:43.724180Z",
     "start_time": "2024-09-23T10:24:34.975084Z"
    }
   },
   "source": [
    "losses = run_experiment(\n",
    "    model_gen=lambda: MLP(784, 128, 10, init_fn=init_std_normal, norm=None),\n",
    "    optim_gen=lambda x: torch.optim.SGD(x.parameters(), lr=0.01),\n",
    "    seed=42,\n",
    "    n_epochs=10,\n",
    "    max_batches=100,\n",
    "    verbose=True,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss = 12.6168\n",
      "Epoch 0 test loss = 9.9327\n",
      "Epoch 1 train loss = 9.0954\n",
      "Epoch 1 test loss = 7.5498\n",
      "Epoch 2 train loss = 6.9607\n",
      "Epoch 2 test loss = 6.2342\n",
      "Epoch 3 train loss = 5.8992\n",
      "Epoch 3 test loss = 5.3655\n",
      "Epoch 4 train loss = 4.9951\n",
      "Epoch 4 test loss = 4.7433\n",
      "Epoch 5 train loss = 4.4778\n",
      "Epoch 5 test loss = 4.3001\n",
      "Epoch 6 train loss = 3.9693\n",
      "Epoch 6 test loss = 3.9605\n",
      "Epoch 7 train loss = 3.7261\n",
      "Epoch 7 test loss = 3.6844\n",
      "Epoch 8 train loss = 3.4223\n",
      "Epoch 8 test loss = 3.4538\n",
      "Epoch 9 train loss = 2.9975\n",
      "Epoch 9 test loss = 3.2638\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства задания настроек эксперимента можно определять их с помощью класса `Experiment`, в котором можно также реализовать логику для строкового представления:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:24:43.745722Z",
     "start_time": "2024-09-23T10:24:43.735389Z"
    }
   },
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 128\n",
    "output_dim = len(train_dataset.classes)\n",
    "\n",
    "\n",
    "class Experiment(NamedTuple):\n",
    "    init_fn: Callable[[nn.Module], None]\n",
    "    act_fn: Callable[[Tensor], Tensor]\n",
    "    norm: Type[nn.BatchNorm1d | nn.LayerNorm] | None\n",
    "    optim_cls: Type[torch.optim.Optimizer]\n",
    "\n",
    "    @property\n",
    "    def model_gen(self) -> Callable[[], nn.Module]:\n",
    "        return lambda: MLP(\n",
    "            input_dim, hidden_dim, output_dim, init_fn=self.init_fn, norm=self.norm\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def optim_gen(self) -> Callable[[nn.Module], torch.optim.Optimizer]:\n",
    "        return lambda x: self.optim_cls(x.parameters(), lr=0.01)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        norm = None if self.norm is None else self.norm.__name__\n",
    "        exp_params = (f\"Experiment parameters: \"\n",
    "                      f\"{self.init_fn.__name__}, {self.act_fn.__name__}, {norm}, {self.optim_cls.__name__}\")\n",
    "        return exp_params"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описываем все эксперименты:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:24:43.767891Z",
     "start_time": "2024-09-23T10:24:43.756754Z"
    }
   },
   "source": [
    "# Все описано дальше\n",
    "\n",
    "'''\n",
    "options = [\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.tanh,\n",
    "        norm=None,\n",
    "        optim_cls=torch.optim.SGD,\n",
    "    ),\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.silu,\n",
    "        norm=nn.LayerNorm,\n",
    "        optim_cls=torch.optim.SGD,\n",
    "    ),\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.relu,\n",
    "        norm=nn.BatchNorm1d,\n",
    "        optim_cls=torch.optim.RMSprop,\n",
    "    ),\n",
    "]\n",
    "'''"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptions = [\\n    Experiment(\\n        init_fn=init_std_normal,\\n        act_fn=F.tanh,\\n        norm=None,\\n        optim_cls=torch.optim.SGD,\\n    ),\\n    Experiment(\\n        init_fn=init_std_normal,\\n        act_fn=F.silu,\\n        norm=nn.LayerNorm,\\n        optim_cls=torch.optim.SGD,\\n    ),\\n    Experiment(\\n        init_fn=init_std_normal,\\n        act_fn=F.relu,\\n        norm=nn.BatchNorm1d,\\n        optim_cls=torch.optim.RMSprop,\\n    ),\\n]\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:24:43.802136Z",
     "start_time": "2024-09-23T10:24:43.795214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "def init_kaiming(model: nn.Module) -> None: \n",
    "    for param in model.parameters():\n",
    "        if param.dim() > 1:\n",
    "            nn.init.kaiming_normal_(param.data, mode='fan_in', nonlinearity='relu')\n",
    "        else:\n",
    "            nn.init.zeros_(param.data)\n",
    "\n",
    "\n",
    "# Сбор различных параметров \n",
    "inits = [init_std_normal, init_kaiming]\n",
    "acts = [F.tanh, F.silu, F.relu]\n",
    "norms = [None, nn.BatchNorm1d, nn.LayerNorm]\n",
    "optims = [torch.optim.SGD, torch.optim.RMSprop, torch.optim.Adam]\n",
    "\n",
    "full_exp_options = list(itertools.product(inits, acts, norms, optims))\n",
    "\n",
    "exp_options = []\n",
    "for option in full_exp_options:\n",
    "    exp_options.append(Experiment(*option))"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Запускаем расчёты:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T11:19:52.024297Z",
     "start_time": "2024-09-23T10:24:43.845683Z"
    }
   },
   "source": [
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "\n",
    "seeds = [42, 56, 12]  # здесь вам нужно 3 разных значения\n",
    " \n",
    "full_options = list(itertools.product(exp_options, seeds))\n",
    "\n",
    "def get_result(option, seed):        \n",
    "    loss = run_experiment(\n",
    "        model_gen=lambda: option.model_gen(),\n",
    "        optim_gen=lambda x: option.optim_gen(x),\n",
    "        seed=seed,\n",
    "        n_epochs=10,\n",
    "        max_batches=None,\n",
    "        verbose=True,\n",
    "    )    \n",
    "    return [str(option), seed, loss]\n",
    "\n",
    "with Parallel(n_jobs=-2, verbose=10) as parallel:\n",
    "    results = parallel(delayed(get_result)(option[0], option[1]) for option in full_options)\n",
    "      "
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-2)]: Done  11 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-2)]: Done  18 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-2)]: Done  27 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-2)]: Done  47 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-2)]: Done  58 tasks      | elapsed: 20.8min\n",
      "[Parallel(n_jobs=-2)]: Done  71 tasks      | elapsed: 25.3min\n",
      "[Parallel(n_jobs=-2)]: Done  84 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=-2)]: Done  99 tasks      | elapsed: 34.6min\n",
      "[Parallel(n_jobs=-2)]: Done 114 tasks      | elapsed: 39.5min\n",
      "[Parallel(n_jobs=-2)]: Done 131 tasks      | elapsed: 45.1min\n",
      "[Parallel(n_jobs=-2)]: Done 148 tasks      | elapsed: 51.3min\n",
      "[Parallel(n_jobs=-2)]: Done 162 out of 162 | elapsed: 55.1min finished\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T11:19:52.084706Z",
     "start_time": "2024-09-23T11:19:52.074312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Вариант кода без распараллеливания \n",
    "\n",
    "\"\"\"\n",
    "seeds = [42, 56, 12]  # здесь вам нужно 3 разных значения\n",
    "results = []\n",
    "\n",
    "for option in options:\n",
    "    print(option)\n",
    "    for seed in seeds:\n",
    "        loss = run_experiment(\n",
    "            model_gen=lambda: option.model_gen(),\n",
    "            # model_gen=option.model_gen(),\n",
    "            optim_gen=lambda x: option.optim_gen(x),\n",
    "            # optim_gen=option.optim_gen(option.model_gen()),\n",
    "            seed=seed,\n",
    "            n_epochs=10,\n",
    "            max_batches=None,\n",
    "            verbose=True,\n",
    "        )\n",
    "        results.append([str(option), seed, loss])   \n",
    "\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nseeds = [42, 56, 12]  # здесь вам нужно 3 разных значения\\nresults = []\\n\\nfor option in options:\\n    print(option)\\n    for seed in seeds:\\n        loss = run_experiment(\\n            model_gen=lambda: option.model_gen(),\\n            # model_gen=option.model_gen(),\\n            optim_gen=lambda x: option.optim_gen(x),\\n            # optim_gen=option.optim_gen(option.model_gen()),\\n            seed=seed,\\n            n_epochs=10,\\n            max_batches=None,\\n            verbose=True,\\n        )\\n        results.append([str(option), seed, loss])   \\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "    Выводим результаты:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T11:27:15.264325Z",
     "start_time": "2024-09-23T11:27:15.222300Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# преобразование описания эксперимента для разнесения параметров на разные колонки\n",
    "for i in range(len(results)):\n",
    "    results[i] = results[i][0][23:].split(\", \") + results[i][1:]\n",
    "\n",
    "df_results = pd.DataFrame(results, \n",
    "             columns=[\"Initialization function\", \"Activation function\", \n",
    "                      \"Normalization function\", \"Optimization function\", \n",
    "                      \"Seed\", \"Loss\"]).sort_values(\"Loss\")\n",
    "\n",
    "df_results"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Initialization function Activation function Normalization function  \\\n",
       "103                                        tanh              LayerNorm   \n",
       "157                                        relu              LayerNorm   \n",
       "130                                        silu              LayerNorm   \n",
       "102                                        tanh              LayerNorm   \n",
       "156                                        relu              LayerNorm   \n",
       "129                                        silu              LayerNorm   \n",
       "48                                         silu              LayerNorm   \n",
       "21                                         tanh              LayerNorm   \n",
       "75                                         relu              LayerNorm   \n",
       "50                                         silu              LayerNorm   \n",
       "23                                         tanh              LayerNorm   \n",
       "77                                         relu              LayerNorm   \n",
       "133                                        silu              LayerNorm   \n",
       "106                                        tanh              LayerNorm   \n",
       "160                                        relu              LayerNorm   \n",
       "161                                        relu              LayerNorm   \n",
       "134                                        silu              LayerNorm   \n",
       "107                                        tanh              LayerNorm   \n",
       "131                                        silu              LayerNorm   \n",
       "104                                        tanh              LayerNorm   \n",
       "158                                        relu              LayerNorm   \n",
       "24                                         tanh              LayerNorm   \n",
       "78                                         relu              LayerNorm   \n",
       "51                                         silu              LayerNorm   \n",
       "124                                        silu            BatchNorm1d   \n",
       "97                                         tanh            BatchNorm1d   \n",
       "151                                        relu            BatchNorm1d   \n",
       "132                                        silu              LayerNorm   \n",
       "105                                        tanh              LayerNorm   \n",
       "159                                        relu              LayerNorm   \n",
       "148                                        relu            BatchNorm1d   \n",
       "121                                        silu            BatchNorm1d   \n",
       "94                                         tanh            BatchNorm1d   \n",
       "79                                         relu              LayerNorm   \n",
       "25                                         tanh              LayerNorm   \n",
       "52                                         silu              LayerNorm   \n",
       "42                                         silu            BatchNorm1d   \n",
       "15                                         tanh            BatchNorm1d   \n",
       "69                                         relu            BatchNorm1d   \n",
       "125                                        silu            BatchNorm1d   \n",
       "98                                         tanh            BatchNorm1d   \n",
       "152                                        relu            BatchNorm1d   \n",
       "76                                         relu              LayerNorm   \n",
       "22                                         tanh              LayerNorm   \n",
       "49                                         silu              LayerNorm   \n",
       "147                                        relu            BatchNorm1d   \n",
       "120                                        silu            BatchNorm1d   \n",
       "93                                         tanh            BatchNorm1d   \n",
       "122                                        silu            BatchNorm1d   \n",
       "95                                         tanh            BatchNorm1d   \n",
       "149                                        relu            BatchNorm1d   \n",
       "96                                         tanh            BatchNorm1d   \n",
       "150                                        relu            BatchNorm1d   \n",
       "123                                        silu            BatchNorm1d   \n",
       "14                                         tanh            BatchNorm1d   \n",
       "41                                         silu            BatchNorm1d   \n",
       "68                                         relu            BatchNorm1d   \n",
       "88                                         tanh                   None   \n",
       "115                                        silu                   None   \n",
       "142                                        relu                   None   \n",
       "111                                        silu                   None   \n",
       "84                                         tanh                   None   \n",
       "138                                        relu                   None   \n",
       "12                                         tanh            BatchNorm1d   \n",
       "66                                         relu            BatchNorm1d   \n",
       "39                                         silu            BatchNorm1d   \n",
       "112                                        silu                   None   \n",
       "85                                         tanh                   None   \n",
       "139                                        relu                   None   \n",
       "116                                        silu                   None   \n",
       "89                                         tanh                   None   \n",
       "143                                        relu                   None   \n",
       "16                                         tanh            BatchNorm1d   \n",
       "43                                         silu            BatchNorm1d   \n",
       "70                                         relu            BatchNorm1d   \n",
       "67                                         relu            BatchNorm1d   \n",
       "13                                         tanh            BatchNorm1d   \n",
       "40                                         silu            BatchNorm1d   \n",
       "71                                         relu            BatchNorm1d   \n",
       "17                                         tanh            BatchNorm1d   \n",
       "44                                         silu            BatchNorm1d   \n",
       "32                                         silu                   None   \n",
       "5                                          tanh                   None   \n",
       "59                                         relu                   None   \n",
       "3                                          tanh                   None   \n",
       "57                                         relu                   None   \n",
       "30                                         silu                   None   \n",
       "80                                         relu              LayerNorm   \n",
       "53                                         silu              LayerNorm   \n",
       "26                                         tanh              LayerNorm   \n",
       "87                                         tanh                   None   \n",
       "141                                        relu                   None   \n",
       "114                                        silu                   None   \n",
       "34                                         silu                   None   \n",
       "7                                          tanh                   None   \n",
       "61                                         relu                   None   \n",
       "128                                        silu              LayerNorm   \n",
       "155                                        relu              LayerNorm   \n",
       "101                                        tanh              LayerNorm   \n",
       "154                                        relu              LayerNorm   \n",
       "127                                        silu              LayerNorm   \n",
       "100                                        tanh              LayerNorm   \n",
       "137                                        relu                   None   \n",
       "110                                        silu                   None   \n",
       "83                                         tanh                   None   \n",
       "99                                         tanh              LayerNorm   \n",
       "126                                        silu              LayerNorm   \n",
       "153                                        relu              LayerNorm   \n",
       "82                                         tanh                   None   \n",
       "136                                        relu                   None   \n",
       "109                                        silu                   None   \n",
       "135                                        relu                   None   \n",
       "108                                        silu                   None   \n",
       "81                                         tanh                   None   \n",
       "60                                         relu                   None   \n",
       "6                                          tanh                   None   \n",
       "33                                         silu                   None   \n",
       "86                                         tanh                   None   \n",
       "140                                        relu                   None   \n",
       "113                                        silu                   None   \n",
       "35                                         silu                   None   \n",
       "62                                         relu                   None   \n",
       "8                                          tanh                   None   \n",
       "145                                        relu            BatchNorm1d   \n",
       "118                                        silu            BatchNorm1d   \n",
       "91                                         tanh            BatchNorm1d   \n",
       "119                                        silu            BatchNorm1d   \n",
       "146                                        relu            BatchNorm1d   \n",
       "92                                         tanh            BatchNorm1d   \n",
       "117                                        silu            BatchNorm1d   \n",
       "90                                         tanh            BatchNorm1d   \n",
       "144                                        relu            BatchNorm1d   \n",
       "31                                         silu                   None   \n",
       "58                                         relu                   None   \n",
       "4                                          tanh                   None   \n",
       "47                                         silu              LayerNorm   \n",
       "20                                         tanh              LayerNorm   \n",
       "74                                         relu              LayerNorm   \n",
       "72                                         relu              LayerNorm   \n",
       "45                                         silu              LayerNorm   \n",
       "18                                         tanh              LayerNorm   \n",
       "36                                         silu            BatchNorm1d   \n",
       "63                                         relu            BatchNorm1d   \n",
       "9                                          tanh            BatchNorm1d   \n",
       "19                                         tanh              LayerNorm   \n",
       "46                                         silu              LayerNorm   \n",
       "73                                         relu              LayerNorm   \n",
       "38                                         silu            BatchNorm1d   \n",
       "65                                         relu            BatchNorm1d   \n",
       "11                                         tanh            BatchNorm1d   \n",
       "37                                         silu            BatchNorm1d   \n",
       "64                                         relu            BatchNorm1d   \n",
       "10                                         tanh            BatchNorm1d   \n",
       "27                                         silu                   None   \n",
       "54                                         relu                   None   \n",
       "0                                          tanh                   None   \n",
       "28                                         silu                   None   \n",
       "55                                         relu                   None   \n",
       "1                                          tanh                   None   \n",
       "29                                         silu                   None   \n",
       "56                                         relu                   None   \n",
       "2                                          tanh                   None   \n",
       "\n",
       "    Optimization function  Seed      Loss  \n",
       "103               RMSprop    56  0.109345  \n",
       "157               RMSprop    56  0.109345  \n",
       "130               RMSprop    56  0.109345  \n",
       "102               RMSprop    42  0.116760  \n",
       "156               RMSprop    42  0.116760  \n",
       "129               RMSprop    42  0.116760  \n",
       "48                RMSprop    42  0.117856  \n",
       "21                RMSprop    42  0.117856  \n",
       "75                RMSprop    42  0.117856  \n",
       "50                RMSprop    12  0.121310  \n",
       "23                RMSprop    12  0.121310  \n",
       "77                RMSprop    12  0.121310  \n",
       "133                  Adam    56  0.121653  \n",
       "106                  Adam    56  0.121653  \n",
       "160                  Adam    56  0.121653  \n",
       "161                  Adam    12  0.129875  \n",
       "134                  Adam    12  0.129875  \n",
       "107                  Adam    12  0.129875  \n",
       "131               RMSprop    12  0.131263  \n",
       "104               RMSprop    12  0.131263  \n",
       "158               RMSprop    12  0.131263  \n",
       "24                   Adam    42  0.133310  \n",
       "78                   Adam    42  0.133310  \n",
       "51                   Adam    42  0.133310  \n",
       "124                  Adam    56  0.136542  \n",
       "97                   Adam    56  0.136542  \n",
       "151                  Adam    56  0.136542  \n",
       "132                  Adam    42  0.137919  \n",
       "105                  Adam    42  0.137919  \n",
       "159                  Adam    42  0.137919  \n",
       "148               RMSprop    56  0.138295  \n",
       "121               RMSprop    56  0.138295  \n",
       "94                RMSprop    56  0.138295  \n",
       "79                   Adam    56  0.141876  \n",
       "25                   Adam    56  0.141876  \n",
       "52                   Adam    56  0.141876  \n",
       "42                   Adam    42  0.145836  \n",
       "15                   Adam    42  0.145836  \n",
       "69                   Adam    42  0.145836  \n",
       "125                  Adam    12  0.146912  \n",
       "98                   Adam    12  0.146912  \n",
       "152                  Adam    12  0.146912  \n",
       "76                RMSprop    56  0.147085  \n",
       "22                RMSprop    56  0.147085  \n",
       "49                RMSprop    56  0.147085  \n",
       "147               RMSprop    42  0.150004  \n",
       "120               RMSprop    42  0.150004  \n",
       "93                RMSprop    42  0.150004  \n",
       "122               RMSprop    12  0.150960  \n",
       "95                RMSprop    12  0.150960  \n",
       "149               RMSprop    12  0.150960  \n",
       "96                   Adam    42  0.151648  \n",
       "150                  Adam    42  0.151648  \n",
       "123                  Adam    42  0.151648  \n",
       "14                RMSprop    12  0.156196  \n",
       "41                RMSprop    12  0.156196  \n",
       "68                RMSprop    12  0.156196  \n",
       "88                   Adam    56  0.160271  \n",
       "115                  Adam    56  0.160271  \n",
       "142                  Adam    56  0.160271  \n",
       "111               RMSprop    42  0.161144  \n",
       "84                RMSprop    42  0.161144  \n",
       "138               RMSprop    42  0.161144  \n",
       "12                RMSprop    42  0.161186  \n",
       "66                RMSprop    42  0.161186  \n",
       "39                RMSprop    42  0.161186  \n",
       "112               RMSprop    56  0.162594  \n",
       "85                RMSprop    56  0.162594  \n",
       "139               RMSprop    56  0.162594  \n",
       "116                  Adam    12  0.163565  \n",
       "89                   Adam    12  0.163565  \n",
       "143                  Adam    12  0.163565  \n",
       "16                   Adam    56  0.164186  \n",
       "43                   Adam    56  0.164186  \n",
       "70                   Adam    56  0.164186  \n",
       "67                RMSprop    56  0.165979  \n",
       "13                RMSprop    56  0.165979  \n",
       "40                RMSprop    56  0.165979  \n",
       "71                   Adam    12  0.166336  \n",
       "17                   Adam    12  0.166336  \n",
       "44                   Adam    12  0.166336  \n",
       "32                RMSprop    12  0.168092  \n",
       "5                 RMSprop    12  0.168092  \n",
       "59                RMSprop    12  0.168092  \n",
       "3                 RMSprop    42  0.177688  \n",
       "57                RMSprop    42  0.177688  \n",
       "30                RMSprop    42  0.177688  \n",
       "80                   Adam    12  0.179001  \n",
       "53                   Adam    12  0.179001  \n",
       "26                   Adam    12  0.179001  \n",
       "87                   Adam    42  0.185049  \n",
       "141                  Adam    42  0.185049  \n",
       "114                  Adam    42  0.185049  \n",
       "34                   Adam    56  0.185451  \n",
       "7                    Adam    56  0.185451  \n",
       "61                   Adam    56  0.185451  \n",
       "128                   SGD    12  0.186533  \n",
       "155                   SGD    12  0.186533  \n",
       "101                   SGD    12  0.186533  \n",
       "154                   SGD    56  0.190380  \n",
       "127                   SGD    56  0.190380  \n",
       "100                   SGD    56  0.190380  \n",
       "137                   SGD    12  0.191442  \n",
       "110                   SGD    12  0.191442  \n",
       "83                    SGD    12  0.191442  \n",
       "99                    SGD    42  0.194827  \n",
       "126                   SGD    42  0.194827  \n",
       "153                   SGD    42  0.194827  \n",
       "82                    SGD    56  0.195122  \n",
       "136                   SGD    56  0.195122  \n",
       "109                   SGD    56  0.195122  \n",
       "135                   SGD    42  0.197545  \n",
       "108                   SGD    42  0.197545  \n",
       "81                    SGD    42  0.197545  \n",
       "60                   Adam    42  0.198490  \n",
       "6                    Adam    42  0.198490  \n",
       "33                   Adam    42  0.198490  \n",
       "86                RMSprop    12  0.214797  \n",
       "140               RMSprop    12  0.214797  \n",
       "113               RMSprop    12  0.214797  \n",
       "35                   Adam    12  0.217229  \n",
       "62                   Adam    12  0.217229  \n",
       "8                    Adam    12  0.217229  \n",
       "145                   SGD    56  0.230293  \n",
       "118                   SGD    56  0.230293  \n",
       "91                    SGD    56  0.230293  \n",
       "119                   SGD    12  0.232897  \n",
       "146                   SGD    12  0.232897  \n",
       "92                    SGD    12  0.232897  \n",
       "117                   SGD    42  0.234870  \n",
       "90                    SGD    42  0.234870  \n",
       "144                   SGD    42  0.234870  \n",
       "31                RMSprop    56  0.260855  \n",
       "58                RMSprop    56  0.260855  \n",
       "4                 RMSprop    56  0.260855  \n",
       "47                    SGD    12  0.451811  \n",
       "20                    SGD    12  0.451811  \n",
       "74                    SGD    12  0.451811  \n",
       "72                    SGD    42  0.455568  \n",
       "45                    SGD    42  0.455568  \n",
       "18                    SGD    42  0.455568  \n",
       "36                    SGD    42  0.464949  \n",
       "63                    SGD    42  0.464949  \n",
       "9                     SGD    42  0.464949  \n",
       "19                    SGD    56  0.467108  \n",
       "46                    SGD    56  0.467108  \n",
       "73                    SGD    56  0.467108  \n",
       "38                    SGD    12  0.479708  \n",
       "65                    SGD    12  0.479708  \n",
       "11                    SGD    12  0.479708  \n",
       "37                    SGD    56  0.484816  \n",
       "64                    SGD    56  0.484816  \n",
       "10                    SGD    56  0.484816  \n",
       "27                    SGD    42  0.634049  \n",
       "54                    SGD    42  0.634049  \n",
       "0                     SGD    42  0.634049  \n",
       "28                    SGD    56  0.643496  \n",
       "55                    SGD    56  0.643496  \n",
       "1                     SGD    56  0.643496  \n",
       "29                    SGD    12  0.693835  \n",
       "56                    SGD    12  0.693835  \n",
       "2                     SGD    12  0.693835  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initialization function</th>\n",
       "      <th>Activation function</th>\n",
       "      <th>Normalization function</th>\n",
       "      <th>Optimization function</th>\n",
       "      <th>Seed</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.109345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.109345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.109345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.116760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.116760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.116760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.117856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.117856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.117856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.121310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.121310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.121310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.121653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.121653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.121653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.129875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.129875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.129875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.131263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.131263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.131263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.133310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.133310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.133310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.136542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.136542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.136542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.137919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.137919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.137919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.138295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.138295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.138295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.141876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.141876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.141876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.145836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.145836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.145836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.146912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.146912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.146912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.147085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.147085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.147085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.150004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.150004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.150004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.150960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.150960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.150960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.151648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.151648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.151648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.156196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.156196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.156196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.160271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.160271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.160271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.161144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.161144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.161144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.161186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.161186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.161186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.162594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.162594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.162594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.163565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.163565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.163565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.164186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.164186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.164186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.165979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.165979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.165979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.166336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.166336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.166336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.168092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.168092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.168092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.177688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.177688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.177688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.179001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.179001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.179001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.185049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.185049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.185049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.185451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.185451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.185451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.186533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.186533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.186533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.190380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.190380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.190380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.191442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.191442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.191442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.194827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.194827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.194827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.195122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.195122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.195122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.197545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.197545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.197545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.198490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.198490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.198490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.214797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.214797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.214797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.217229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.217229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.217229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.230293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.230293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.230293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.232897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.232897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.232897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.234870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.234870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.234870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.260855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.260855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.260855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.451811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.451811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.451811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.455568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.455568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.455568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.464949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.464949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.464949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.467108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.467108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.467108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.479708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.479708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.479708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.484816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.484816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.484816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.634049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.634049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.634049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.643496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.643496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.643496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.693835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td></td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.693835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.693835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T11:27:16.754527Z",
     "start_time": "2024-09-23T11:27:16.738165Z"
    }
   },
   "cell_type": "code",
   "source": "print(df_results)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Initialization function Activation function Normalization function  \\\n",
      "103                                        tanh              LayerNorm   \n",
      "157                                        relu              LayerNorm   \n",
      "130                                        silu              LayerNorm   \n",
      "102                                        tanh              LayerNorm   \n",
      "156                                        relu              LayerNorm   \n",
      "129                                        silu              LayerNorm   \n",
      "48                                         silu              LayerNorm   \n",
      "21                                         tanh              LayerNorm   \n",
      "75                                         relu              LayerNorm   \n",
      "50                                         silu              LayerNorm   \n",
      "23                                         tanh              LayerNorm   \n",
      "77                                         relu              LayerNorm   \n",
      "133                                        silu              LayerNorm   \n",
      "106                                        tanh              LayerNorm   \n",
      "160                                        relu              LayerNorm   \n",
      "161                                        relu              LayerNorm   \n",
      "134                                        silu              LayerNorm   \n",
      "107                                        tanh              LayerNorm   \n",
      "131                                        silu              LayerNorm   \n",
      "104                                        tanh              LayerNorm   \n",
      "158                                        relu              LayerNorm   \n",
      "24                                         tanh              LayerNorm   \n",
      "78                                         relu              LayerNorm   \n",
      "51                                         silu              LayerNorm   \n",
      "124                                        silu            BatchNorm1d   \n",
      "97                                         tanh            BatchNorm1d   \n",
      "151                                        relu            BatchNorm1d   \n",
      "132                                        silu              LayerNorm   \n",
      "105                                        tanh              LayerNorm   \n",
      "159                                        relu              LayerNorm   \n",
      "148                                        relu            BatchNorm1d   \n",
      "121                                        silu            BatchNorm1d   \n",
      "94                                         tanh            BatchNorm1d   \n",
      "79                                         relu              LayerNorm   \n",
      "25                                         tanh              LayerNorm   \n",
      "52                                         silu              LayerNorm   \n",
      "42                                         silu            BatchNorm1d   \n",
      "15                                         tanh            BatchNorm1d   \n",
      "69                                         relu            BatchNorm1d   \n",
      "125                                        silu            BatchNorm1d   \n",
      "98                                         tanh            BatchNorm1d   \n",
      "152                                        relu            BatchNorm1d   \n",
      "76                                         relu              LayerNorm   \n",
      "22                                         tanh              LayerNorm   \n",
      "49                                         silu              LayerNorm   \n",
      "147                                        relu            BatchNorm1d   \n",
      "120                                        silu            BatchNorm1d   \n",
      "93                                         tanh            BatchNorm1d   \n",
      "122                                        silu            BatchNorm1d   \n",
      "95                                         tanh            BatchNorm1d   \n",
      "149                                        relu            BatchNorm1d   \n",
      "96                                         tanh            BatchNorm1d   \n",
      "150                                        relu            BatchNorm1d   \n",
      "123                                        silu            BatchNorm1d   \n",
      "14                                         tanh            BatchNorm1d   \n",
      "41                                         silu            BatchNorm1d   \n",
      "68                                         relu            BatchNorm1d   \n",
      "88                                         tanh                   None   \n",
      "115                                        silu                   None   \n",
      "142                                        relu                   None   \n",
      "111                                        silu                   None   \n",
      "84                                         tanh                   None   \n",
      "138                                        relu                   None   \n",
      "12                                         tanh            BatchNorm1d   \n",
      "66                                         relu            BatchNorm1d   \n",
      "39                                         silu            BatchNorm1d   \n",
      "112                                        silu                   None   \n",
      "85                                         tanh                   None   \n",
      "139                                        relu                   None   \n",
      "116                                        silu                   None   \n",
      "89                                         tanh                   None   \n",
      "143                                        relu                   None   \n",
      "16                                         tanh            BatchNorm1d   \n",
      "43                                         silu            BatchNorm1d   \n",
      "70                                         relu            BatchNorm1d   \n",
      "67                                         relu            BatchNorm1d   \n",
      "13                                         tanh            BatchNorm1d   \n",
      "40                                         silu            BatchNorm1d   \n",
      "71                                         relu            BatchNorm1d   \n",
      "17                                         tanh            BatchNorm1d   \n",
      "44                                         silu            BatchNorm1d   \n",
      "32                                         silu                   None   \n",
      "5                                          tanh                   None   \n",
      "59                                         relu                   None   \n",
      "3                                          tanh                   None   \n",
      "57                                         relu                   None   \n",
      "30                                         silu                   None   \n",
      "80                                         relu              LayerNorm   \n",
      "53                                         silu              LayerNorm   \n",
      "26                                         tanh              LayerNorm   \n",
      "87                                         tanh                   None   \n",
      "141                                        relu                   None   \n",
      "114                                        silu                   None   \n",
      "34                                         silu                   None   \n",
      "7                                          tanh                   None   \n",
      "61                                         relu                   None   \n",
      "128                                        silu              LayerNorm   \n",
      "155                                        relu              LayerNorm   \n",
      "101                                        tanh              LayerNorm   \n",
      "154                                        relu              LayerNorm   \n",
      "127                                        silu              LayerNorm   \n",
      "100                                        tanh              LayerNorm   \n",
      "137                                        relu                   None   \n",
      "110                                        silu                   None   \n",
      "83                                         tanh                   None   \n",
      "99                                         tanh              LayerNorm   \n",
      "126                                        silu              LayerNorm   \n",
      "153                                        relu              LayerNorm   \n",
      "82                                         tanh                   None   \n",
      "136                                        relu                   None   \n",
      "109                                        silu                   None   \n",
      "135                                        relu                   None   \n",
      "108                                        silu                   None   \n",
      "81                                         tanh                   None   \n",
      "60                                         relu                   None   \n",
      "6                                          tanh                   None   \n",
      "33                                         silu                   None   \n",
      "86                                         tanh                   None   \n",
      "140                                        relu                   None   \n",
      "113                                        silu                   None   \n",
      "35                                         silu                   None   \n",
      "62                                         relu                   None   \n",
      "8                                          tanh                   None   \n",
      "145                                        relu            BatchNorm1d   \n",
      "118                                        silu            BatchNorm1d   \n",
      "91                                         tanh            BatchNorm1d   \n",
      "119                                        silu            BatchNorm1d   \n",
      "146                                        relu            BatchNorm1d   \n",
      "92                                         tanh            BatchNorm1d   \n",
      "117                                        silu            BatchNorm1d   \n",
      "90                                         tanh            BatchNorm1d   \n",
      "144                                        relu            BatchNorm1d   \n",
      "31                                         silu                   None   \n",
      "58                                         relu                   None   \n",
      "4                                          tanh                   None   \n",
      "47                                         silu              LayerNorm   \n",
      "20                                         tanh              LayerNorm   \n",
      "74                                         relu              LayerNorm   \n",
      "72                                         relu              LayerNorm   \n",
      "45                                         silu              LayerNorm   \n",
      "18                                         tanh              LayerNorm   \n",
      "36                                         silu            BatchNorm1d   \n",
      "63                                         relu            BatchNorm1d   \n",
      "9                                          tanh            BatchNorm1d   \n",
      "19                                         tanh              LayerNorm   \n",
      "46                                         silu              LayerNorm   \n",
      "73                                         relu              LayerNorm   \n",
      "38                                         silu            BatchNorm1d   \n",
      "65                                         relu            BatchNorm1d   \n",
      "11                                         tanh            BatchNorm1d   \n",
      "37                                         silu            BatchNorm1d   \n",
      "64                                         relu            BatchNorm1d   \n",
      "10                                         tanh            BatchNorm1d   \n",
      "27                                         silu                   None   \n",
      "54                                         relu                   None   \n",
      "0                                          tanh                   None   \n",
      "28                                         silu                   None   \n",
      "55                                         relu                   None   \n",
      "1                                          tanh                   None   \n",
      "29                                         silu                   None   \n",
      "56                                         relu                   None   \n",
      "2                                          tanh                   None   \n",
      "\n",
      "    Optimization function  Seed      Loss  \n",
      "103               RMSprop    56  0.109345  \n",
      "157               RMSprop    56  0.109345  \n",
      "130               RMSprop    56  0.109345  \n",
      "102               RMSprop    42  0.116760  \n",
      "156               RMSprop    42  0.116760  \n",
      "129               RMSprop    42  0.116760  \n",
      "48                RMSprop    42  0.117856  \n",
      "21                RMSprop    42  0.117856  \n",
      "75                RMSprop    42  0.117856  \n",
      "50                RMSprop    12  0.121310  \n",
      "23                RMSprop    12  0.121310  \n",
      "77                RMSprop    12  0.121310  \n",
      "133                  Adam    56  0.121653  \n",
      "106                  Adam    56  0.121653  \n",
      "160                  Adam    56  0.121653  \n",
      "161                  Adam    12  0.129875  \n",
      "134                  Adam    12  0.129875  \n",
      "107                  Adam    12  0.129875  \n",
      "131               RMSprop    12  0.131263  \n",
      "104               RMSprop    12  0.131263  \n",
      "158               RMSprop    12  0.131263  \n",
      "24                   Adam    42  0.133310  \n",
      "78                   Adam    42  0.133310  \n",
      "51                   Adam    42  0.133310  \n",
      "124                  Adam    56  0.136542  \n",
      "97                   Adam    56  0.136542  \n",
      "151                  Adam    56  0.136542  \n",
      "132                  Adam    42  0.137919  \n",
      "105                  Adam    42  0.137919  \n",
      "159                  Adam    42  0.137919  \n",
      "148               RMSprop    56  0.138295  \n",
      "121               RMSprop    56  0.138295  \n",
      "94                RMSprop    56  0.138295  \n",
      "79                   Adam    56  0.141876  \n",
      "25                   Adam    56  0.141876  \n",
      "52                   Adam    56  0.141876  \n",
      "42                   Adam    42  0.145836  \n",
      "15                   Adam    42  0.145836  \n",
      "69                   Adam    42  0.145836  \n",
      "125                  Adam    12  0.146912  \n",
      "98                   Adam    12  0.146912  \n",
      "152                  Adam    12  0.146912  \n",
      "76                RMSprop    56  0.147085  \n",
      "22                RMSprop    56  0.147085  \n",
      "49                RMSprop    56  0.147085  \n",
      "147               RMSprop    42  0.150004  \n",
      "120               RMSprop    42  0.150004  \n",
      "93                RMSprop    42  0.150004  \n",
      "122               RMSprop    12  0.150960  \n",
      "95                RMSprop    12  0.150960  \n",
      "149               RMSprop    12  0.150960  \n",
      "96                   Adam    42  0.151648  \n",
      "150                  Adam    42  0.151648  \n",
      "123                  Adam    42  0.151648  \n",
      "14                RMSprop    12  0.156196  \n",
      "41                RMSprop    12  0.156196  \n",
      "68                RMSprop    12  0.156196  \n",
      "88                   Adam    56  0.160271  \n",
      "115                  Adam    56  0.160271  \n",
      "142                  Adam    56  0.160271  \n",
      "111               RMSprop    42  0.161144  \n",
      "84                RMSprop    42  0.161144  \n",
      "138               RMSprop    42  0.161144  \n",
      "12                RMSprop    42  0.161186  \n",
      "66                RMSprop    42  0.161186  \n",
      "39                RMSprop    42  0.161186  \n",
      "112               RMSprop    56  0.162594  \n",
      "85                RMSprop    56  0.162594  \n",
      "139               RMSprop    56  0.162594  \n",
      "116                  Adam    12  0.163565  \n",
      "89                   Adam    12  0.163565  \n",
      "143                  Adam    12  0.163565  \n",
      "16                   Adam    56  0.164186  \n",
      "43                   Adam    56  0.164186  \n",
      "70                   Adam    56  0.164186  \n",
      "67                RMSprop    56  0.165979  \n",
      "13                RMSprop    56  0.165979  \n",
      "40                RMSprop    56  0.165979  \n",
      "71                   Adam    12  0.166336  \n",
      "17                   Adam    12  0.166336  \n",
      "44                   Adam    12  0.166336  \n",
      "32                RMSprop    12  0.168092  \n",
      "5                 RMSprop    12  0.168092  \n",
      "59                RMSprop    12  0.168092  \n",
      "3                 RMSprop    42  0.177688  \n",
      "57                RMSprop    42  0.177688  \n",
      "30                RMSprop    42  0.177688  \n",
      "80                   Adam    12  0.179001  \n",
      "53                   Adam    12  0.179001  \n",
      "26                   Adam    12  0.179001  \n",
      "87                   Adam    42  0.185049  \n",
      "141                  Adam    42  0.185049  \n",
      "114                  Adam    42  0.185049  \n",
      "34                   Adam    56  0.185451  \n",
      "7                    Adam    56  0.185451  \n",
      "61                   Adam    56  0.185451  \n",
      "128                   SGD    12  0.186533  \n",
      "155                   SGD    12  0.186533  \n",
      "101                   SGD    12  0.186533  \n",
      "154                   SGD    56  0.190380  \n",
      "127                   SGD    56  0.190380  \n",
      "100                   SGD    56  0.190380  \n",
      "137                   SGD    12  0.191442  \n",
      "110                   SGD    12  0.191442  \n",
      "83                    SGD    12  0.191442  \n",
      "99                    SGD    42  0.194827  \n",
      "126                   SGD    42  0.194827  \n",
      "153                   SGD    42  0.194827  \n",
      "82                    SGD    56  0.195122  \n",
      "136                   SGD    56  0.195122  \n",
      "109                   SGD    56  0.195122  \n",
      "135                   SGD    42  0.197545  \n",
      "108                   SGD    42  0.197545  \n",
      "81                    SGD    42  0.197545  \n",
      "60                   Adam    42  0.198490  \n",
      "6                    Adam    42  0.198490  \n",
      "33                   Adam    42  0.198490  \n",
      "86                RMSprop    12  0.214797  \n",
      "140               RMSprop    12  0.214797  \n",
      "113               RMSprop    12  0.214797  \n",
      "35                   Adam    12  0.217229  \n",
      "62                   Adam    12  0.217229  \n",
      "8                    Adam    12  0.217229  \n",
      "145                   SGD    56  0.230293  \n",
      "118                   SGD    56  0.230293  \n",
      "91                    SGD    56  0.230293  \n",
      "119                   SGD    12  0.232897  \n",
      "146                   SGD    12  0.232897  \n",
      "92                    SGD    12  0.232897  \n",
      "117                   SGD    42  0.234870  \n",
      "90                    SGD    42  0.234870  \n",
      "144                   SGD    42  0.234870  \n",
      "31                RMSprop    56  0.260855  \n",
      "58                RMSprop    56  0.260855  \n",
      "4                 RMSprop    56  0.260855  \n",
      "47                    SGD    12  0.451811  \n",
      "20                    SGD    12  0.451811  \n",
      "74                    SGD    12  0.451811  \n",
      "72                    SGD    42  0.455568  \n",
      "45                    SGD    42  0.455568  \n",
      "18                    SGD    42  0.455568  \n",
      "36                    SGD    42  0.464949  \n",
      "63                    SGD    42  0.464949  \n",
      "9                     SGD    42  0.464949  \n",
      "19                    SGD    56  0.467108  \n",
      "46                    SGD    56  0.467108  \n",
      "73                    SGD    56  0.467108  \n",
      "38                    SGD    12  0.479708  \n",
      "65                    SGD    12  0.479708  \n",
      "11                    SGD    12  0.479708  \n",
      "37                    SGD    56  0.484816  \n",
      "64                    SGD    56  0.484816  \n",
      "10                    SGD    56  0.484816  \n",
      "27                    SGD    42  0.634049  \n",
      "54                    SGD    42  0.634049  \n",
      "0                     SGD    42  0.634049  \n",
      "28                    SGD    56  0.643496  \n",
      "55                    SGD    56  0.643496  \n",
      "1                     SGD    56  0.643496  \n",
      "29                    SGD    12  0.693835  \n",
      "56                    SGD    12  0.693835  \n",
      "2                     SGD    12  0.693835  \n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОДЫ:\n",
    "\n",
    "Из полученной таблицы явно видно, что все лучшие результаты (для любого сида) получены при использовании LayerNorm и RMSprop. При таком выборе нормализации и оптимизации лучшей функцией инициализации показала себя Kaimimg Normal, однако стандартное нормальное распределение (init_std_normal) при инициализации лишь незначительно хуже. Касательно функции активации вывод сделать невозможно, так как явных зависимостей в сравнении с другими параметрами не прослеживается.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-mcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
