{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализайия и нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит реализовать два вида нормализации: по батчам (BatchNorm1d) и по признакам (LayerNorm1d)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:23:48.806069Z",
     "start_time": "2024-09-23T20:23:43.657743Z"
    }
   },
   "source": [
    "from typing import Callable, NamedTuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Реализация BatchNorm1d и LayerNorm1d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 1.1. (2 балла) Реализуйте BatchNorm1d\n",
    "\n",
    "Подсказка: чтобы хранить текущие значения среднего и дисперсии, вам потребуется метод `torch.nn.Module.register_buffer`, ознакомьтесь с документацией к нему. Подумайте, какие проблемы возникнут, если вы будете просто сохранять ваши значения в тензор"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:23:48.821589Z",
     "start_time": "2024-09-23T20:23:48.813102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BatchNorm1d(nn.Module):\n",
    "    def __init__(self, num_features: int, momentum: float = 0.9, eps: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(num_features))\n",
    "        self.shift = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_var\", torch.ones(num_features))\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(dim=0)\n",
    "            batch_var = x.var(dim=0)\n",
    "\n",
    "            self.running_mean = (1 - self.momentum) * batch_mean + self.momentum * self.running_mean\n",
    "            self.running_var = (1 - self.momentum) * batch_var + self.momentum * self.running_var\n",
    "\n",
    "            x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "        else:\n",
    "            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
    "\n",
    "        return self.scale * x_normalized + self.shift"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. (1 балл) Реализуйте LayerNorm1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия LayerNorm от BatchNorm - в том, что расчёт средних и дисперсий в BatchNorm происходит вдоль размерности батча (см. рисунок слева), а в LayerNorm - вдоль размерности признаков (см. рисунок справа).\n",
    "\n",
    "<img src=\"norm.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:23:48.902410Z",
     "start_time": "2024-09-23T20:23:48.894823Z"
    }
   },
   "source": [
    "class LayerNorm1d(nn.Module):\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5) -> None:\n",
    "        super(LayerNorm1d, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(num_features))\n",
    "        self.shift = nn.Parameter(torch.zeros(num_features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        \n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        return self.scale * x_normalized + self.shift"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании ваша задача - проверить, какие из приёмов хорошо справляются с нездоровыми активациями в промежуточных слоях. Вам будет дана базовая модель, у которой есть проблемы с инициализацией параметров, попробуйте несколько приёмов для устранения проблем обучения:\n",
    "1. Хорошая инициализация параметров\n",
    "2. Ненасыщаемая функция активации (например, `F.leaky_relu`)\n",
    "3. Нормализация по батчам или по признакам (можно использовать встроенные `nn.BatchNorm1d` и `nn.LayerNorm`)\n",
    "4. Более продвинутый оптимизатор (`torch.optim.RMSprop`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0. Подготовка: датасет, функции для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверять наши гипотезы будем на датасете MNIST, для отладки добавим в функции для обучения возможность использовать только несколько батчей данных"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:23:50.911291Z",
     "start_time": "2024-09-23T20:23:48.909825Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:23:50.929943Z",
     "start_time": "2024-09-23T20:23:50.921963Z"
    }
   },
   "source": [
    "def training_step(\n",
    "    batch: tuple[torch.Tensor, torch.Tensor],\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> torch.Tensor:\n",
    "    # прогоняем батч через модель\n",
    "    x, y = batch\n",
    "    logits = model(x)\n",
    "    # оцениваем значение ошибки\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    # обновляем параметры\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # возвращаем значение функции ошибки для логирования\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    max_batches: int = 100,\n",
    ") -> Tensor:\n",
    "    model.train()\n",
    "    loss_values: list[float] = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        loss = training_step(batch, model, optimizer)\n",
    "        loss_values.append(loss.item())\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    return torch.tensor(loss_values).mean()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_epoch(\n",
    "    dataloader: DataLoader, model: nn.Module, max_batches: int = 100\n",
    ") -> Tensor:\n",
    "    model.eval()\n",
    "    loss_values: list[float] = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = batch\n",
    "        logits = model(x)\n",
    "        # оцениваем значение ошибки\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss_values.append(loss.item())\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    return torch.tensor(loss_values).mean()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Определение класса модели (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства проведения экспериментов мы немного усложним создание модели, чтобы можно было задать разные способы инициализации параметров и нормализации промежуточных активаций, не меняя определение класса.\n",
    "\n",
    "Добавьте в метод `__init__`:\n",
    "- аргумент, который позволит использовать разные функции активации для промежуточных слоёв\n",
    "- аргумент, который позволит задавать разные способы нормализации: `None` (без нормализации), `nn.BatchNorm` и `nn.LayerNorm`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:23:50.946713Z",
     "start_time": "2024-09-23T20:23:50.937792Z"
    }
   },
   "source": [
    "def init_std_normal(model: nn.Module) -> None:\n",
    "    \"\"\"Функция для инициализации параметров модели стандартным нормальным распределением.\"\"\"\n",
    "    for param in model.parameters():\n",
    "        torch.nn.init.normal_(param.data, mean=0, std=1)\n",
    "\n",
    "\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Базовая модель для экспериментов\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): размерность входных признаков\n",
    "        hidden_dim (int): размерност скрытого слоя\n",
    "        output_dim (int): кол-во классов\n",
    "        act_fn (Callable[[Tensor], Tensor], optional): Функция активации. Defaults to F.tanh.\n",
    "        init_fn (Callable[[nn.Module], None], optional): Функция для инициализации. Defaults to init_std_normal.\n",
    "        norm (Type[nn.BatchNorm1d  |  nn.LayerNorm] | None, optional): Способ нормализации промежуточных активаций.\n",
    "            Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        act_fn: Callable[[Tensor], Tensor] = F.tanh,\n",
    "        init_fn: Callable[[nn.Module], None] = init_std_normal,\n",
    "        norm: Type[nn.BatchNorm1d | nn.LayerNorm] | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # теперь линейные слои будем задавать\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act_fn = act_fn\n",
    "        self.norm = norm(hidden_dim) if norm else None\n",
    "\n",
    "        # reinitialize parameters\n",
    "        init_fn(self)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = self.fc1.forward(x.flatten(1))\n",
    "        # here you can do normalization\n",
    "        if self.norm:\n",
    "            h = self.norm(h)\n",
    "        return self.fc2.forward(self.act_fn(h))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Эксперименты (7 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите по 3 эксперимента с каждой из модификаций с разными значениями `seed`, соберите статистику значений тестовой ошибки после 10 эпох обучения, сделайте выводы о том, что работает лучше\n",
    "\n",
    "Проверяем:\n",
    "1. Метод инициализации весов модели: $\\mathcal{N}(0, 1)$ / Kaiming normal\n",
    "2. Функция активации: tanh /  (или любая другая без насыщения)\n",
    "3. Слой нормализации: None / BatchNorm / LayerNorm\n",
    "4. Выбранный оптимизатор: SGD / RMSprop / Adam\n",
    "\n",
    "Итого у нас 2 + 2 + 3 + 3 = 10 экспериментов, каждый нужно повторить 3 раза, посчитать среднее и вывести результаты в pandas.DataFrame.\n",
    "Можно дополнительно потестировать разные сочетания опций, например инициализация + нормализация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы автоматизировать проведение экспериментов, можно использовать функцию, которая будет принимать все необходимые настройки эксперимента, запускать его и сохранять нужные метрики:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:23:50.966432Z",
     "start_time": "2024-09-23T20:23:50.955528Z"
    }
   },
   "source": [
    "def run_experiment(\n",
    "    model_gen: Callable[[], nn.Module],\n",
    "    optim_gen: Callable[[nn.Module], torch.optim.Optimizer],\n",
    "    seed: int,\n",
    "    n_epochs: int = 10,\n",
    "    max_batches: int | None = None,\n",
    "    verbose: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"Функция для запуска экспериментов.\n",
    "\n",
    "    Args:\n",
    "        model_gen (Callable[[], nn.Module]): Функция для создания модели\n",
    "        optim_gen (Callable[[nn.Module], torch.optim.Optimizer]): Функция для создания оптимизатора для модели\n",
    "        seed (int): random seed\n",
    "        n_epochs (int, optional): Число эпох обучения. Defaults to 10.\n",
    "        max_batches (int | None, optional): Если указано, только `max_batches` минибатчей\n",
    "            будет использоваться при обучении и тестировании. Defaults to None.\n",
    "        verbose (bool, optional): Выводить ли информацию для отладки. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        float: Значение ошибки на тестовой выборке в конце обучения\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    # создадим модель и выведем значение ошибки после инициализации\n",
    "    model = model_gen()\n",
    "    optim = optim_gen(model)\n",
    "    epoch_losses: list[float] = []\n",
    "    for i in range(n_epochs):\n",
    "        train_loss = train_epoch(train_loader, model, optim, max_batches=max_batches)\n",
    "        test_loss = test_epoch(test_loader, model, max_batches=max_batches)\n",
    "        if verbose:\n",
    "            print(f\"Epoch {i} train loss = {train_loss:.4f}\")\n",
    "            print(f\"Epoch {i} test loss = {test_loss:.4f}\")\n",
    "\n",
    "        epoch_losses.append(test_loss.item())\n",
    "\n",
    "    last_epoch_loss = epoch_losses[-1]\n",
    "    return last_epoch_loss"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример использования:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:23:57.949432Z",
     "start_time": "2024-09-23T20:23:50.974815Z"
    }
   },
   "source": [
    "losses = run_experiment(\n",
    "    model_gen=lambda: MLP(784, 128, 10, init_fn=init_std_normal, norm=None),\n",
    "    optim_gen=lambda x: torch.optim.SGD(x.parameters(), lr=0.01),\n",
    "    seed=42,\n",
    "    n_epochs=10,\n",
    "    max_batches=100,\n",
    "    verbose=True,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss = 12.6168\n",
      "Epoch 0 test loss = 9.9327\n",
      "Epoch 1 train loss = 9.0954\n",
      "Epoch 1 test loss = 7.5498\n",
      "Epoch 2 train loss = 6.9607\n",
      "Epoch 2 test loss = 6.2342\n",
      "Epoch 3 train loss = 5.8992\n",
      "Epoch 3 test loss = 5.3655\n",
      "Epoch 4 train loss = 4.9951\n",
      "Epoch 4 test loss = 4.7433\n",
      "Epoch 5 train loss = 4.4778\n",
      "Epoch 5 test loss = 4.3001\n",
      "Epoch 6 train loss = 3.9693\n",
      "Epoch 6 test loss = 3.9605\n",
      "Epoch 7 train loss = 3.7261\n",
      "Epoch 7 test loss = 3.6844\n",
      "Epoch 8 train loss = 3.4223\n",
      "Epoch 8 test loss = 3.4538\n",
      "Epoch 9 train loss = 2.9975\n",
      "Epoch 9 test loss = 3.2638\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства задания настроек эксперимента можно определять их с помощью класса `Experiment`, в котором можно также реализовать логику для строкового представления:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:23:57.972464Z",
     "start_time": "2024-09-23T20:23:57.960381Z"
    }
   },
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 128\n",
    "output_dim = len(train_dataset.classes)\n",
    "\n",
    "\n",
    "class Experiment(NamedTuple):\n",
    "    init_fn: Callable[[nn.Module], None]\n",
    "    act_fn: Callable[[Tensor], Tensor]\n",
    "    norm: Type[nn.BatchNorm1d | nn.LayerNorm] | None\n",
    "    optim_cls: Type[torch.optim.Optimizer]\n",
    "\n",
    "    @property\n",
    "    def model_gen(self) -> Callable[[], nn.Module]:\n",
    "        return lambda: MLP(\n",
    "            input_dim, hidden_dim, output_dim, init_fn=self.init_fn, norm=self.norm\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def optim_gen(self) -> Callable[[nn.Module], torch.optim.Optimizer]:\n",
    "        return lambda x: self.optim_cls(x.parameters(), lr=0.01)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        norm = None if self.norm is None else self.norm.__name__\n",
    "        exp_params = (f\"Experiment parameters: \"\n",
    "                      f\"{self.init_fn.__name__}, {self.act_fn.__name__}, {norm}, {self.optim_cls.__name__}\")\n",
    "        return exp_params"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описываем все эксперименты:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:23:57.994164Z",
     "start_time": "2024-09-23T20:23:57.981440Z"
    }
   },
   "source": [
    "# Все описано дальше\n",
    "\n",
    "'''\n",
    "options = [\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.tanh,\n",
    "        norm=None,\n",
    "        optim_cls=torch.optim.SGD,\n",
    "    ),\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.silu,\n",
    "        norm=nn.LayerNorm,\n",
    "        optim_cls=torch.optim.SGD,\n",
    "    ),\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.relu,\n",
    "        norm=nn.BatchNorm1d,\n",
    "        optim_cls=torch.optim.RMSprop,\n",
    "    ),\n",
    "]\n",
    "'''"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptions = [\\n    Experiment(\\n        init_fn=init_std_normal,\\n        act_fn=F.tanh,\\n        norm=None,\\n        optim_cls=torch.optim.SGD,\\n    ),\\n    Experiment(\\n        init_fn=init_std_normal,\\n        act_fn=F.silu,\\n        norm=nn.LayerNorm,\\n        optim_cls=torch.optim.SGD,\\n    ),\\n    Experiment(\\n        init_fn=init_std_normal,\\n        act_fn=F.relu,\\n        norm=nn.BatchNorm1d,\\n        optim_cls=torch.optim.RMSprop,\\n    ),\\n]\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:23:58.021346Z",
     "start_time": "2024-09-23T20:23:58.006834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "def init_kaiming(model: nn.Module) -> None: \n",
    "    for param in model.parameters():\n",
    "        if param.dim() > 1:\n",
    "            nn.init.kaiming_normal_(param.data, mode='fan_in', nonlinearity='relu')\n",
    "        else:\n",
    "            nn.init.zeros_(param.data)\n",
    "\n",
    "\n",
    "# Сбор различных параметров \n",
    "inits = [init_std_normal, init_kaiming]\n",
    "acts = [F.tanh, F.silu, F.relu]\n",
    "norms = [None, nn.BatchNorm1d, nn.LayerNorm]\n",
    "optims = [torch.optim.SGD, torch.optim.RMSprop, torch.optim.Adam]\n",
    "\n",
    "full_exp_options = list(itertools.product(inits, acts, norms, optims))\n",
    "\n",
    "exp_options = []\n",
    "for option in full_exp_options:\n",
    "    exp_options.append(Experiment(*option))"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Запускаем расчёты:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T21:22:52.217013Z",
     "start_time": "2024-09-23T20:23:58.051904Z"
    }
   },
   "source": [
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "\n",
    "seeds = [42, 56, 12]  # здесь вам нужно 3 разных значения\n",
    " \n",
    "full_options = list(itertools.product(exp_options, seeds))\n",
    "\n",
    "def get_result(option, seed):        \n",
    "    loss = run_experiment(\n",
    "        model_gen=lambda: option.model_gen(),\n",
    "        optim_gen=lambda x: option.optim_gen(x),\n",
    "        seed=seed,\n",
    "        n_epochs=10,\n",
    "        max_batches=None,\n",
    "        verbose=True,\n",
    "    )    \n",
    "    return [str(option), seed, loss]\n",
    "\n",
    "with Parallel(n_jobs=-3, verbose=10) as parallel:\n",
    "    results = parallel(delayed(get_result)(option[0], option[1]) for option in full_options)\n",
    "      "
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done   1 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-3)]: Done   6 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-3)]: Done  13 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-3)]: Done  20 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-3)]: Done  29 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=-3)]: Done  38 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-3)]: Done  49 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-3)]: Done  60 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=-3)]: Done  73 tasks      | elapsed: 29.0min\n",
      "[Parallel(n_jobs=-3)]: Done  86 tasks      | elapsed: 33.4min\n",
      "[Parallel(n_jobs=-3)]: Done 101 tasks      | elapsed: 37.8min\n",
      "[Parallel(n_jobs=-3)]: Done 116 tasks      | elapsed: 44.0min\n",
      "[Parallel(n_jobs=-3)]: Done 133 tasks      | elapsed: 50.3min\n",
      "[Parallel(n_jobs=-3)]: Done 150 tasks      | elapsed: 54.9min\n",
      "[Parallel(n_jobs=-3)]: Done 162 out of 162 | elapsed: 58.9min finished\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T21:22:52.250758Z",
     "start_time": "2024-09-23T21:22:52.236696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Вариант кода без распараллеливания \n",
    "\n",
    "\"\"\"\n",
    "seeds = [42, 56, 12]  # здесь вам нужно 3 разных значения\n",
    "results = []\n",
    "\n",
    "for option in options:\n",
    "    print(option)\n",
    "    for seed in seeds:\n",
    "        loss = run_experiment(\n",
    "            model_gen=lambda: option.model_gen(),\n",
    "            # model_gen=option.model_gen(),\n",
    "            optim_gen=lambda x: option.optim_gen(x),\n",
    "            # optim_gen=option.optim_gen(option.model_gen()),\n",
    "            seed=seed,\n",
    "            n_epochs=10,\n",
    "            max_batches=None,\n",
    "            verbose=True,\n",
    "        )\n",
    "        results.append([str(option), seed, loss])   \n",
    "\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nseeds = [42, 56, 12]  # здесь вам нужно 3 разных значения\\nresults = []\\n\\nfor option in options:\\n    print(option)\\n    for seed in seeds:\\n        loss = run_experiment(\\n            model_gen=lambda: option.model_gen(),\\n            # model_gen=option.model_gen(),\\n            optim_gen=lambda x: option.optim_gen(x),\\n            # optim_gen=option.optim_gen(option.model_gen()),\\n            seed=seed,\\n            n_epochs=10,\\n            max_batches=None,\\n            verbose=True,\\n        )\\n        results.append([str(option), seed, loss])   \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "    Выводим результаты:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T21:22:52.317701Z",
     "start_time": "2024-09-23T21:22:52.307557Z"
    }
   },
   "source": [
    "# преобразование описания эксперимента для разнесения параметров на разные колонки\n",
    "for i in range(len(results)):\n",
    "    results[i] = results[i][0][23:].split(\", \") + results[i][1:]\n",
    "\n",
    "# усредняем по seed для одинаковых параметров\n",
    "final_results = []\n",
    "for i in range(0, len(results), 3):\n",
    "    final_results.append(results[i][:4] + [(results[i][5] + results[i+1][5] + results[i+2][5]) / 3])"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T21:22:53.989804Z",
     "start_time": "2024-09-23T21:22:52.329195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_final_results = pd.DataFrame(final_results, \n",
    "             columns=[\"Initialization function\", \"Activation function\", \n",
    "                      \"Normalization function\", \"Optimization function\", \n",
    "                      \"Loss\"]).sort_values(\"Loss\")\n",
    "\n",
    "df_final_results"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Initialization function Activation function Normalization function  \\\n",
       "41            init_kaiming                silu            BatchNorm1d   \n",
       "50            init_kaiming                relu            BatchNorm1d   \n",
       "32            init_kaiming                tanh            BatchNorm1d   \n",
       "49            init_kaiming                relu            BatchNorm1d   \n",
       "31            init_kaiming                tanh            BatchNorm1d   \n",
       "40            init_kaiming                silu            BatchNorm1d   \n",
       "13         init_std_normal                silu            BatchNorm1d   \n",
       "22         init_std_normal                relu            BatchNorm1d   \n",
       "4          init_std_normal                tanh            BatchNorm1d   \n",
       "23         init_std_normal                relu            BatchNorm1d   \n",
       "5          init_std_normal                tanh            BatchNorm1d   \n",
       "14         init_std_normal                silu            BatchNorm1d   \n",
       "43            init_kaiming                silu              LayerNorm   \n",
       "34            init_kaiming                tanh              LayerNorm   \n",
       "52            init_kaiming                relu              LayerNorm   \n",
       "16         init_std_normal                silu              LayerNorm   \n",
       "25         init_std_normal                relu              LayerNorm   \n",
       "7          init_std_normal                tanh              LayerNorm   \n",
       "53            init_kaiming                relu              LayerNorm   \n",
       "44            init_kaiming                silu              LayerNorm   \n",
       "35            init_kaiming                tanh              LayerNorm   \n",
       "26         init_std_normal                relu              LayerNorm   \n",
       "17         init_std_normal                silu              LayerNorm   \n",
       "8          init_std_normal                tanh              LayerNorm   \n",
       "47            init_kaiming                relu                   None   \n",
       "29            init_kaiming                tanh                   None   \n",
       "38            init_kaiming                silu                   None   \n",
       "37            init_kaiming                silu                   None   \n",
       "28            init_kaiming                tanh                   None   \n",
       "46            init_kaiming                relu                   None   \n",
       "51            init_kaiming                relu              LayerNorm   \n",
       "42            init_kaiming                silu              LayerNorm   \n",
       "33            init_kaiming                tanh              LayerNorm   \n",
       "27            init_kaiming                tanh                   None   \n",
       "45            init_kaiming                relu                   None   \n",
       "36            init_kaiming                silu                   None   \n",
       "2          init_std_normal                tanh                   None   \n",
       "20         init_std_normal                relu                   None   \n",
       "11         init_std_normal                silu                   None   \n",
       "10         init_std_normal                silu                   None   \n",
       "1          init_std_normal                tanh                   None   \n",
       "19         init_std_normal                relu                   None   \n",
       "39            init_kaiming                silu            BatchNorm1d   \n",
       "30            init_kaiming                tanh            BatchNorm1d   \n",
       "48            init_kaiming                relu            BatchNorm1d   \n",
       "12         init_std_normal                silu            BatchNorm1d   \n",
       "3          init_std_normal                tanh            BatchNorm1d   \n",
       "21         init_std_normal                relu            BatchNorm1d   \n",
       "6          init_std_normal                tanh              LayerNorm   \n",
       "15         init_std_normal                silu              LayerNorm   \n",
       "24         init_std_normal                relu              LayerNorm   \n",
       "9          init_std_normal                silu                   None   \n",
       "18         init_std_normal                relu                   None   \n",
       "0          init_std_normal                tanh                   None   \n",
       "\n",
       "   Optimization function      Loss  \n",
       "41                  Adam  0.101984  \n",
       "50                  Adam  0.101984  \n",
       "32                  Adam  0.101984  \n",
       "49               RMSprop  0.102871  \n",
       "31               RMSprop  0.102871  \n",
       "40               RMSprop  0.102871  \n",
       "13               RMSprop  0.108138  \n",
       "22               RMSprop  0.108138  \n",
       "4                RMSprop  0.108138  \n",
       "23                  Adam  0.111855  \n",
       "5                   Adam  0.111855  \n",
       "14                  Adam  0.111855  \n",
       "43               RMSprop  0.119123  \n",
       "34               RMSprop  0.119123  \n",
       "52               RMSprop  0.119123  \n",
       "16               RMSprop  0.128750  \n",
       "25               RMSprop  0.128750  \n",
       "7                RMSprop  0.128750  \n",
       "53                  Adam  0.129816  \n",
       "44                  Adam  0.129816  \n",
       "35                  Adam  0.129816  \n",
       "26                  Adam  0.151396  \n",
       "17                  Adam  0.151396  \n",
       "8                   Adam  0.151396  \n",
       "47                  Adam  0.169628  \n",
       "29                  Adam  0.169628  \n",
       "38                  Adam  0.169628  \n",
       "37               RMSprop  0.179512  \n",
       "28               RMSprop  0.179512  \n",
       "46               RMSprop  0.179512  \n",
       "51                   SGD  0.190580  \n",
       "42                   SGD  0.190580  \n",
       "33                   SGD  0.190580  \n",
       "27                   SGD  0.194703  \n",
       "45                   SGD  0.194703  \n",
       "36                   SGD  0.194703  \n",
       "2                   Adam  0.200390  \n",
       "20                  Adam  0.200390  \n",
       "11                  Adam  0.200390  \n",
       "10               RMSprop  0.202212  \n",
       "1                RMSprop  0.202212  \n",
       "19               RMSprop  0.202212  \n",
       "39                   SGD  0.220309  \n",
       "30                   SGD  0.220309  \n",
       "48                   SGD  0.220309  \n",
       "12                   SGD  0.439329  \n",
       "3                    SGD  0.439329  \n",
       "21                   SGD  0.439329  \n",
       "6                    SGD  0.458162  \n",
       "15                   SGD  0.458162  \n",
       "24                   SGD  0.458162  \n",
       "9                    SGD  0.657127  \n",
       "18                   SGD  0.657127  \n",
       "0                    SGD  0.657127  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initialization function</th>\n",
       "      <th>Activation function</th>\n",
       "      <th>Normalization function</th>\n",
       "      <th>Optimization function</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.101984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.101984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.101984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.102871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.102871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.102871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.108138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.108138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.108138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.111855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.111855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.111855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.119123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.119123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.119123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.128750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.128750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.128750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.129816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.129816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.129816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.151396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.151396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.151396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.169628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.169628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.169628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.179512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.179512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.179512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.190580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.190580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.190580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.194703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.194703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.194703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.200390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.200390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.200390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.202212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.202212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.202212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.220309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.220309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.220309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>silu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.439329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.439329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>relu</td>\n",
       "      <td>BatchNorm1d</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.439329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.458162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>silu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.458162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.458162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>silu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.657127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>relu</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.657127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.657127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОДЫ:\n",
    "\n",
    "Из полученной таблицы явно видно, что все лучшие результаты (для любого сида) получены при использовании BatchNorm1d и Adam. При таком выборе нормализации и оптимизации лучшей функцией инициализации показала себя Kaimimg Normal, однако стандартное нормальное распределение (init_std_normal) при инициализации лишь незначительно хуже, как и аналогичные параметры с оптимизатором RMSprop. Касательно функции активации вывод сделать невозможно, так как явных зависимостей в сравнении с другими параметрами не прослеживается.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-mcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
