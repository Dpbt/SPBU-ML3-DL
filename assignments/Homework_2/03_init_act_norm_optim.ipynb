{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализайия и нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит реализовать два вида нормализации: по батчам (BatchNorm1d) и по признакам (LayerNorm1d)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:28:01.257927Z",
     "start_time": "2024-09-22T21:28:01.250265Z"
    }
   },
   "source": [
    "from typing import Callable, NamedTuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Реализация BatchNorm1d и LayerNorm1d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #### 1.1. (2 балла) Реализуйте BatchNorm1d\n",
    "\n",
    "Подсказка: чтобы хранить текущие значения среднего и дисперсии, вам потребуется метод `torch.nn.Module.register_buffer`, ознакомьтесь с документацией к нему. Подумайте, какие проблемы возникнут, если вы будете просто сохранять ваши значения в тензор"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:28:01.322345Z",
     "start_time": "2024-09-22T21:28:01.314259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BatchNorm1d(nn.Module):\n",
    "    def __init__(self, num_features: int, momentum: float = 0.9, eps: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(num_features))\n",
    "        self.shift = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_var\", torch.ones(num_features))\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(dim=0)\n",
    "            batch_var = x.var(dim=0)\n",
    "\n",
    "            self.running_mean = (1 - self.momentum) * batch_mean + self.momentum * self.running_mean\n",
    "            self.running_var = (1 - self.momentum) * batch_var + self.momentum * self.running_var\n",
    "\n",
    "            x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
    "        else:\n",
    "            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
    "\n",
    "        return self.scale * x_normalized + self.shift"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. (1 балл) Реализуйте LayerNorm1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия LayerNorm от BatchNorm - в том, что расчёт средних и дисперсий в BatchNorm происходит вдоль размерности батча (см. рисунок слева), а в LayerNorm - вдоль размерности признаков (см. рисунок справа).\n",
    "\n",
    "<img src=\"norm.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:28:01.335851Z",
     "start_time": "2024-09-22T21:28:01.322345Z"
    }
   },
   "source": [
    "class LayerNorm1d(nn.Module):\n",
    "    def __init__(self, num_features: int, eps: float = 1e-5) -> None:\n",
    "        super(LayerNorm1d, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(num_features))\n",
    "        self.shift = nn.Parameter(torch.zeros(num_features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        \n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        return self.scale * x_normalized + self.shift"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании ваша задача - проверить, какие из приёмов хорошо справляются с нездоровыми активациями в промежуточных слоях. Вам будет дана базовая модель, у которой есть проблемы с инициализацией параметров, попробуйте несколько приёмов для устранения проблем обучения:\n",
    "1. Хорошая инициализация параметров\n",
    "2. Ненасыщаемая функция активации (например, `F.leaky_relu`)\n",
    "3. Нормализация по батчам или по признакам (можно использовать встроенные `nn.BatchNorm1d` и `nn.LayerNorm`)\n",
    "4. Более продвинутый оптимизатор (`torch.optim.RMSprop`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0. Подготовка: датасет, функции для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверять наши гипотезы будем на датасете MNIST, для отладки добавим в функции для обучения возможность использовать только несколько батчей данных"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:28:01.410777Z",
     "start_time": "2024-09-22T21:28:01.348752Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:28:01.427923Z",
     "start_time": "2024-09-22T21:28:01.419245Z"
    }
   },
   "source": [
    "def training_step(\n",
    "    batch: tuple[torch.Tensor, torch.Tensor],\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> torch.Tensor:\n",
    "    # прогоняем батч через модель\n",
    "    x, y = batch\n",
    "    logits = model(x)\n",
    "    # оцениваем значение ошибки\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    # обновляем параметры\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # возвращаем значение функции ошибки для логирования\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    dataloader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    max_batches: int = 100,\n",
    ") -> Tensor:\n",
    "    loss_values: list[float] = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        loss = training_step(batch, model, optimizer)\n",
    "        loss_values.append(loss.item())\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    return torch.tensor(loss_values).mean()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_epoch(\n",
    "    dataloader: DataLoader, model: nn.Module, max_batches: int = 100\n",
    ") -> Tensor:\n",
    "    loss_values: list[float] = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = batch\n",
    "        logits = model(x)\n",
    "        # оцениваем значение ошибки\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss_values.append(loss.item())\n",
    "        if i == max_batches:\n",
    "            break\n",
    "    return torch.tensor(loss_values).mean()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Определение класса модели (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства проведения экспериментов мы немного усложним создание модели, чтобы можно было задать разные способы инициализации параметров и нормализации промежуточных активаций, не меняя определение класса.\n",
    "\n",
    "Добавьте в метод `__init__`:\n",
    "- аргумент, который позволит использовать разные функции активации для промежуточных слоёв\n",
    "- аргумент, который позволит задавать разные способы нормализации: `None` (без нормализации), `nn.BatchNorm` и `nn.LayerNorm`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:28:01.445989Z",
     "start_time": "2024-09-22T21:28:01.435574Z"
    }
   },
   "source": [
    "def init_std_normal(model: nn.Module) -> None:\n",
    "    \"\"\"Функция для инициализации параметров модели стандартным нормальным распределением.\"\"\"\n",
    "    for param in model.parameters():\n",
    "        torch.nn.init.normal_(param.data, mean=0, std=1)\n",
    "\n",
    "\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Базовая модель для экспериментов\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): размерность входных признаков\n",
    "        hidden_dim (int): размерност скрытого слоя\n",
    "        output_dim (int): кол-во классов\n",
    "        act_fn (Callable[[Tensor], Tensor], optional): Функция активации. Defaults to F.tanh.\n",
    "        init_fn (Callable[[nn.Module], None], optional): Функция для инициализации. Defaults to init_std_normal.\n",
    "        norm (Type[nn.BatchNorm1d  |  nn.LayerNorm] | None, optional): Способ нормализации промежуточных активаций.\n",
    "            Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        act_fn: Callable[[Tensor], Tensor] = F.tanh,\n",
    "        init_fn: Callable[[nn.Module], None] = init_std_normal,\n",
    "        norm: Type[nn.BatchNorm1d | nn.LayerNorm] | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # теперь линейные слои будем задавать\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act_fn = act_fn\n",
    "        self.norm = norm(hidden_dim) if norm else None\n",
    "\n",
    "        # reinitialize parameters\n",
    "        init_fn(self)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = self.fc1.forward(x.flatten(1))\n",
    "        # here you can do normalization\n",
    "        if self.norm:\n",
    "            h = self.norm(h)\n",
    "        return self.fc2.forward(self.act_fn(h))"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Эксперименты (7 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите по 3 эксперимента с каждой из модификаций с разными значениями `seed`, соберите статистику значений тестовой ошибки после 10 эпох обучения, сделайте выводы о том, что работает лучше\n",
    "\n",
    "Проверяем:\n",
    "1. Метод инициализации весов модели: $\\mathcal{N}(0, 1)$ / Kaiming normal\n",
    "2. Функция активации: tanh /  (или любая другая без насыщения)\n",
    "3. Слой нормализации: None / BatchNorm / LayerNorm\n",
    "4. Выбранный оптимизатор: SGD / RMSprop / Adam\n",
    "\n",
    "Итого у нас 2 + 2 + 3 + 3 = 10 экспериментов, каждый нужно повторить 3 раза, посчитать среднее и вывести результаты в pandas.DataFrame.\n",
    "Можно дополнительно потестировать разные сочетания опций, например инициализация + нормализация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы автоматизировать проведение экспериментов, можно использовать функцию, которая будет принимать все необходимые настройки эксперимента, запускать его и сохранять нужные метрики:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:28:01.462485Z",
     "start_time": "2024-09-22T21:28:01.455364Z"
    }
   },
   "source": [
    "def run_experiment(\n",
    "    model_gen: Callable[[], nn.Module],\n",
    "    optim_gen: Callable[[nn.Module], torch.optim.Optimizer],\n",
    "    seed: int,\n",
    "    n_epochs: int = 10,\n",
    "    max_batches: int | None = None,\n",
    "    verbose: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"Функция для запуска экспериментов.\n",
    "\n",
    "    Args:\n",
    "        model_gen (Callable[[], nn.Module]): Функция для создания модели\n",
    "        optim_gen (Callable[[nn.Module], torch.optim.Optimizer]): Функция для создания оптимизатора для модели\n",
    "        seed (int): random seed\n",
    "        n_epochs (int, optional): Число эпох обучения. Defaults to 10.\n",
    "        max_batches (int | None, optional): Если указано, только `max_batches` минибатчей\n",
    "            будет использоваться при обучении и тестировании. Defaults to None.\n",
    "        verbose (bool, optional): Выводить ли информацию для отладки. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        float: Значение ошибки на тестовой выборке в конце обучения\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    # создадим модель и выведем значение ошибки после инициализации\n",
    "    model = model_gen()\n",
    "    optim = optim_gen(model)\n",
    "    epoch_losses: list[float] = []\n",
    "    for i in range(n_epochs):\n",
    "        train_loss = train_epoch(train_loader, model, optim, max_batches=max_batches)\n",
    "        test_loss = test_epoch(test_loader, model, max_batches=max_batches)\n",
    "        if verbose:\n",
    "            print(f\"Epoch {i} train loss = {train_loss:.4f}\")\n",
    "            print(f\"Epoch {i} test loss = {test_loss:.4f}\")\n",
    "\n",
    "        epoch_losses.append(test_loss.item())\n",
    "\n",
    "    last_epoch_loss = epoch_losses[-1]\n",
    "    return last_epoch_loss"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример использования:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:28:08.528541Z",
     "start_time": "2024-09-22T21:28:01.475566Z"
    }
   },
   "source": [
    "losses = run_experiment(\n",
    "    model_gen=lambda: MLP(784, 128, 10, init_fn=init_std_normal, norm=None),\n",
    "    optim_gen=lambda x: torch.optim.SGD(x.parameters(), lr=0.01),\n",
    "    seed=42,\n",
    "    n_epochs=10,\n",
    "    max_batches=100,\n",
    "    verbose=True,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss = 12.6168\n",
      "Epoch 0 test loss = 9.9327\n",
      "Epoch 1 train loss = 9.0954\n",
      "Epoch 1 test loss = 7.5498\n",
      "Epoch 2 train loss = 6.9607\n",
      "Epoch 2 test loss = 6.2342\n",
      "Epoch 3 train loss = 5.8992\n",
      "Epoch 3 test loss = 5.3655\n",
      "Epoch 4 train loss = 4.9951\n",
      "Epoch 4 test loss = 4.7433\n",
      "Epoch 5 train loss = 4.4778\n",
      "Epoch 5 test loss = 4.3001\n",
      "Epoch 6 train loss = 3.9693\n",
      "Epoch 6 test loss = 3.9605\n",
      "Epoch 7 train loss = 3.7261\n",
      "Epoch 7 test loss = 3.6844\n",
      "Epoch 8 train loss = 3.4223\n",
      "Epoch 8 test loss = 3.4538\n",
      "Epoch 9 train loss = 2.9975\n",
      "Epoch 9 test loss = 3.2638\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства задания настроек эксперимента можно определять их с помощью класса `Experiment`, в котором можно также реализовать логику для строкового представления:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:28:08.543777Z",
     "start_time": "2024-09-22T21:28:08.535793Z"
    }
   },
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 128\n",
    "output_dim = len(train_dataset.classes)\n",
    "\n",
    "\n",
    "class Experiment(NamedTuple):\n",
    "    init_fn: Callable[[nn.Module], None]\n",
    "    act_fn: Callable[[Tensor], Tensor]\n",
    "    norm: Type[nn.BatchNorm1d | nn.LayerNorm] | None\n",
    "    optim_cls: Type[torch.optim.Optimizer]\n",
    "\n",
    "    @property\n",
    "    def model_gen(self) -> Callable[[], nn.Module]:\n",
    "        return lambda: MLP(\n",
    "            input_dim, hidden_dim, output_dim, init_fn=self.init_fn, norm=self.norm\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def optim_gen(self) -> Callable[[nn.Module], torch.optim.Optimizer]:\n",
    "        return lambda x: self.optim_cls(x.parameters(), lr=0.01)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        norm = None if self.norm is None else self.norm.__name__\n",
    "        exp_params = (f\"Experiment parameters: \"\n",
    "                      f\"{self.init_fn.__name__}, {self.act_fn.__name__}, {norm}, {self.optim_cls.__name__}\")\n",
    "        return exp_params"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описываем все эксперименты:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:28:08.565230Z",
     "start_time": "2024-09-22T21:28:08.557201Z"
    }
   },
   "source": [
    "# Все описано дальше\n",
    "\n",
    "'''\n",
    "options = [\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.tanh,\n",
    "        norm=None,\n",
    "        optim_cls=torch.optim.SGD,\n",
    "    ),\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.silu,\n",
    "        norm=nn.LayerNorm,\n",
    "        optim_cls=torch.optim.SGD,\n",
    "    ),\n",
    "    Experiment(\n",
    "        init_fn=init_std_normal,\n",
    "        act_fn=F.relu,\n",
    "        norm=nn.BatchNorm1d,\n",
    "        optim_cls=torch.optim.RMSprop,\n",
    "    ),\n",
    "]\n",
    "'''"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptions = [\\n    Experiment(\\n        init_fn=init_std_normal,\\n        act_fn=F.tanh,\\n        norm=None,\\n        optim_cls=torch.optim.SGD,\\n    ),\\n    Experiment(\\n        init_fn=init_std_normal,\\n        act_fn=F.silu,\\n        norm=nn.LayerNorm,\\n        optim_cls=torch.optim.SGD,\\n    ),\\n    Experiment(\\n        init_fn=init_std_normal,\\n        act_fn=F.relu,\\n        norm=nn.BatchNorm1d,\\n        optim_cls=torch.optim.RMSprop,\\n    ),\\n]\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:28:08.591397Z",
     "start_time": "2024-09-22T21:28:08.576399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "\n",
    "def init_kaiming(model: nn.Module) -> None: \n",
    "    for param in model.parameters():\n",
    "        if param.dim() > 1:\n",
    "            nn.init.kaiming_normal_(param.data, mode='fan_in', nonlinearity='relu')\n",
    "        else:\n",
    "            nn.init.zeros_(param.data)\n",
    "\n",
    "\n",
    "# Сбор различных параметров \n",
    "inits = [init_std_normal, init_kaiming]\n",
    "acts = [F.tanh, F.silu, F.relu]\n",
    "norms = [None, nn.BatchNorm1d, nn.LayerNorm]\n",
    "optims = [torch.optim.SGD, torch.optim.RMSprop, torch.optim.Adam]\n",
    "\n",
    "full_exp_options = list(itertools.product(inits, acts, norms, optims))\n",
    "\n",
    "exp_options = []\n",
    "for option in full_exp_options:\n",
    "    exp_options.append(Experiment(*option))"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Запускаем расчёты:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T22:24:52.660893Z",
     "start_time": "2024-09-22T21:28:08.624568Z"
    }
   },
   "source": [
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "\n",
    "seeds = [42, 56, 12]  # здесь вам нужно 3 разных значения\n",
    " \n",
    "full_options = list(itertools.product(exp_options, seeds))\n",
    "\n",
    "def get_result(option, seed):        \n",
    "    loss = run_experiment(\n",
    "        model_gen=lambda: option.model_gen(),\n",
    "        optim_gen=lambda x: option.optim_gen(x),\n",
    "        seed=seed,\n",
    "        n_epochs=10,\n",
    "        max_batches=None,\n",
    "        verbose=True,\n",
    "    )    \n",
    "    return [str(option), seed, loss]\n",
    "\n",
    "with Parallel(n_jobs=-2, verbose=10) as parallel:\n",
    "    results = parallel(delayed(get_result)(option[0], option[1]) for option in full_options)\n",
    "      "
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   4 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-2)]: Done  11 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-2)]: Done  18 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-2)]: Done  27 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-2)]: Done  47 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-2)]: Done  58 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=-2)]: Done  71 tasks      | elapsed: 26.9min\n",
      "[Parallel(n_jobs=-2)]: Done  84 tasks      | elapsed: 30.2min\n",
      "[Parallel(n_jobs=-2)]: Done  99 tasks      | elapsed: 36.2min\n",
      "[Parallel(n_jobs=-2)]: Done 114 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=-2)]: Done 131 tasks      | elapsed: 46.9min\n",
      "[Parallel(n_jobs=-2)]: Done 148 tasks      | elapsed: 53.0min\n",
      "[Parallel(n_jobs=-2)]: Done 162 out of 162 | elapsed: 56.7min finished\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T22:24:52.778777Z",
     "start_time": "2024-09-22T22:24:52.734246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Вариант кода без распараллеливания \n",
    "\n",
    "\"\"\"\n",
    "seeds = [42, 56, 12]  # здесь вам нужно 3 разных значения\n",
    "results = []\n",
    "\n",
    "for option in options:\n",
    "    print(option)\n",
    "    for seed in seeds:\n",
    "        loss = run_experiment(\n",
    "            model_gen=lambda: option.model_gen(),\n",
    "            # model_gen=option.model_gen(),\n",
    "            optim_gen=lambda x: option.optim_gen(x),\n",
    "            # optim_gen=option.optim_gen(option.model_gen()),\n",
    "            seed=seed,\n",
    "            n_epochs=10,\n",
    "            max_batches=None,\n",
    "            verbose=True,\n",
    "        )\n",
    "        results.append([str(option), seed, loss])   \n",
    "\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nseeds = [42, 56, 12]  # здесь вам нужно 3 разных значения\\nresults = []\\n\\nfor option in options:\\n    print(option)\\n    for seed in seeds:\\n        loss = run_experiment(\\n            model_gen=lambda: option.model_gen(),\\n            # model_gen=option.model_gen(),\\n            optim_gen=lambda x: option.optim_gen(x),\\n            # optim_gen=option.optim_gen(option.model_gen()),\\n            seed=seed,\\n            n_epochs=10,\\n            max_batches=None,\\n            verbose=True,\\n        )\\n        results.append([str(option), seed, loss])   \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "    Выводим результаты:"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T22:24:52.969890Z",
     "start_time": "2024-09-22T22:24:52.828598Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# преобразование описания эксперимента для разнесения параметров на разные колонки\n",
    "for i in range(len(results)):\n",
    "    results[i] = results[i][0][23:].split(\", \") + results[i][1:]\n",
    "\n",
    "pd.DataFrame(results, \n",
    "             columns=[\"Initialization function\", \"Activation function\", \n",
    "                      \"Normalization function\", \"Optimization function\", \n",
    "                      \"Seed\", \"Loss\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Initialization function Activation function Normalization function  \\\n",
       "0           init_std_normal                tanh                   None   \n",
       "1           init_std_normal                tanh                   None   \n",
       "2           init_std_normal                tanh                   None   \n",
       "3           init_std_normal                tanh                   None   \n",
       "4           init_std_normal                tanh                   None   \n",
       "..                      ...                 ...                    ...   \n",
       "157            init_kaiming                relu              LayerNorm   \n",
       "158            init_kaiming                relu              LayerNorm   \n",
       "159            init_kaiming                relu              LayerNorm   \n",
       "160            init_kaiming                relu              LayerNorm   \n",
       "161            init_kaiming                relu              LayerNorm   \n",
       "\n",
       "    Optimization function  Seed      Loss  \n",
       "0                     SGD    42  0.634049  \n",
       "1                     SGD    56  0.643496  \n",
       "2                     SGD    12  0.693835  \n",
       "3                 RMSprop    42  0.177688  \n",
       "4                 RMSprop    56  0.260855  \n",
       "..                    ...   ...       ...  \n",
       "157               RMSprop    56  0.109345  \n",
       "158               RMSprop    12  0.131263  \n",
       "159                  Adam    42  0.137919  \n",
       "160                  Adam    56  0.121653  \n",
       "161                  Adam    12  0.129875  \n",
       "\n",
       "[162 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initialization function</th>\n",
       "      <th>Activation function</th>\n",
       "      <th>Normalization function</th>\n",
       "      <th>Optimization function</th>\n",
       "      <th>Seed</th>\n",
       "      <th>Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>42</td>\n",
       "      <td>0.634049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>56</td>\n",
       "      <td>0.643496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>SGD</td>\n",
       "      <td>12</td>\n",
       "      <td>0.693835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>42</td>\n",
       "      <td>0.177688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>init_std_normal</td>\n",
       "      <td>tanh</td>\n",
       "      <td>None</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.260855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>56</td>\n",
       "      <td>0.109345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>12</td>\n",
       "      <td>0.131263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>42</td>\n",
       "      <td>0.137919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>56</td>\n",
       "      <td>0.121653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>init_kaiming</td>\n",
       "      <td>relu</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>Adam</td>\n",
       "      <td>12</td>\n",
       "      <td>0.129875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВЫВОДЫ:\n",
    "\n",
    "Из полученной таблицы явно видно, что все лучшие результаты (для любого сида) получены при использовании LayerNorm и RMSprop. При таком выборе нормализации и оптимизации лучшей функцией инициализации показала себя Kaimimg Normal, однако стандартное нормальное распределение (init_std_normal) при инициализации лишь незначительно хуже. Касательно функции активации вывод сделать невозможно, так как явных зависимостей в сравнении с другими параметрами не прослеживается.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-mcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
